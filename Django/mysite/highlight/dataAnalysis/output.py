# -*- coding: utf-8 -*-
"""usingdata.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12Xz_rbYpOi4XOTX2V2CWPgTHAPE6eF6N
"""

import pandas as pd
import numpy as np
import tensorflow
from sklearn.preprocessing import MinMaxScaler
from matplotlib import pyplot as plt
from keras.models import Sequential
from keras.layers import Dense
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.layers import LSTM
from sklearn.model_selection import train_test_split
import sys
import glob, os
mod = sys.modules[__name__]
from tensorflow.keras.models import load_model

def output(args, df):
    ##################### df 경로 ######################
    # df = pd.read_excel('/content/test/20200427_Faker_604123582.mp4_2.xlsx')

    # del df['Unnamed: 0']
    scaler = MinMaxScaler()
    df.set_index('time', inplace=True)

    df['delta_k']= df['k'] - df['k'].shift(1)
    df['delta_d']= df['d'] - df['d'].shift(1)
    df['delta_a']= df['a'] - df['a'].shift(1)
    df['delta_gold']= df['gold'] - df['gold'].shift(1)

    df['delta_emo'] = abs(df[['ha','sa','an','ca','dis','fe','sup','conf']] -
                                    df[['ha','sa','an','ca','dis','fe','sup','conf']].shift(1)).sum(axis=1)

    df.dropna(inplace=True)

    window_size = 15
    scale_cols = ['k','d','a','gold','ha','sa','an','ca','dis','fe','sup','conf'
                    ,'sound']

    feature_cols = ['k','d','a','gold','ha','sa','an','ca','dis','fe','sup','conf',
                    'sound','delta_k','delta_d','delta_a','delta_emo']

    # label_cols = ['count']

    df[scale_cols] = scaler.fit_transform(df[scale_cols])

    def make_dataset2(data, window_size=20):
        feature_list = []
        
        for i in range(len(data) - window_size):
            feature_list.append(np.array(data.iloc[i:i+window_size]))
            # label_list.append(np.array(label.iloc[i+window_size]))
        return np.array(feature_list)


    test_feature = df[feature_cols]
    # test_label = df[label_cols]

    test_feature = make_dataset2(test_feature, window_size)

    ################## 모델 경로 ###################
    model = load_model('./static/highlight/highlight_final.h5')
    # print('model',os.path.isfile('./static/highlight/highlight_final.h5'))
    pred = model.predict(test_feature)

    print('pred', pred)
    highlight = pd.DataFrame(pred, index=df.index[15:])

    highlight.to_csv(args['data_root'] + 'highlight.csv')
    
    df.to_csv(args['data_root'] + 'final_result.csv')    
    highlight_rate = pred.tolist()
    k_data = df['delta_k'].tolist()
    d_data = df['delta_d'].tolist()
    a_data = df['delta_a'].tolist()

    highlight2 = []
    for i in highlight_rate:
        highlight2.append(i[0])

    print('highlight', highlight2)
    print('k', k_data)
    print('d', d_data)
    print('a', a_data)


    ############# 하이라이트 및 K, D, A 발생지점 ###################
    # return highlight, df['delta_k'], df['delta_d'], df['delta_a']
    return highlight2, k_data, d_data, a_data